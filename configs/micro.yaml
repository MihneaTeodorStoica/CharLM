vocab_size: 256
d_model: 64
n_layer: 2
n_head: 2
n_kv_head: 2
seq_len: 64
dropout: 0.0
warmup_steps: 10
max_steps: 20
batch_size_tokens: 1024
micro_bsz: 4

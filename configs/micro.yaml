model:
  vocab_size: 256
  d_model: 128
  n_layer: 2
  n_head: 4
  n_kv_head: 2
  seq_len: 64
  dropout: 0.0
optimizer:
  lr: 1.0e-3
  weight_decay: 0.01
scheduler:
  warmup_steps: 10
  max_steps: 200
  min_lr: 1.0e-5
batch_size_tokens: 4096
micro_batch_size: 8
eval_interval: 50
eval_batches: 4
logging:
  log_interval: 5
checkpoint:
  out_dir: checkpoints
  save_interval: 50
  keep_last: 2
seed: 0
precision: auto
compile: false

model:
  vocab_size: 256
  d_model: 512
  n_layer: 12
  n_head: 8
  n_kv_head: 2
  seq_len: 1024
  dropout: 0.0
optimizer:
  lr: 3.0e-4
  weight_decay: 0.1
scheduler:
  warmup_steps: 2000
  max_steps: 200000
  min_lr: 1.0e-5
batch_size_tokens: 1048576
micro_batch_size: 8
precision: auto
compile: false
grad_clip: 1.0
seed: 0
eval_interval: 2000
eval_batches: 16
logging:
  log_interval: 100
checkpoint:
  out_dir: checkpoints
  save_interval: 2000
  keep_last: 3
